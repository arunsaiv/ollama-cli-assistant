# ğŸ§  Ollama CLI Chat Assistant

A cross-platform, Python-based **CLI tool** to interact with local LLMs (like LLaMA3) using [Ollama](https://ollama.com/). Get AI responses in your terminal â€” with real-time streaming output and no API key required!

---

## ğŸ¯ Features

- âœ… Chat with local LLMs from the terminal
- âš¡ Real-time **streaming output**
- ğŸ”„ Typing-like effect with a spinner while the model is thinking
- ğŸ–¥ï¸ Works on **macOS, Windows, and Linux**
- ğŸ”“ No OpenAI key or paid API â€” uses Ollama locally

---

## ğŸ§± Requirements

- Python 3.8+
- [Ollama](https://ollama.com/) installed and running (default port: `11434`)
- A supported model like `llama3` pulled via Ollama

---

## ğŸš€ Getting Started

### 1. Clone the repo

```bash
git clone https://github.com/your-username/ollama-cli-chat.git
cd ollama-cli-chat
```

### 2. (Optional) Create a virtual environment

```bash
pip install -r requirements.txt
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Pull the model using Ollama

```bash
ollama pull llama3
```

### 5. Run the chat tool

```bash
python main.py
```

## ğŸ“¸ Demo

```bash
ğŸ’¬ You: What's the capital of France?
ğŸ¤– AI is thinking... |
ğŸ§  AI: The capital of France is Paris.
```

## ğŸ“‚ File Structure

```bash
ollama-cli-chat/
â”œâ”€â”€ main.py              # Entry point
â”œâ”€â”€ requirements.txt     # Python dependencies
â””â”€â”€ README.md            # This file
```

## ğŸ§  How It Works

-	Uses requests to call Ollamaâ€™s generate endpoint
-	Handles real-time streaming token output
-	Displays a spinner while waiting for a response

## ğŸ› ï¸ Future Improvements

-	Support for chat history
-	Model switching during runtime
- Optional markdown rendering

## ğŸ¤ Contributing

Contributions are welcome!
Feel free to fork, suggest improvements, or raise issues.

## ğŸ“„ License

MIT License Â© 2025 [Arun Sai Veerisetty]
